:sectanchors:
:toc: macro
:toclevels: 2
:toc-title: Table of Content
:numbered:

= Hands On Lab with Wildfly Swarm, Microservices & OpenShift

toc::[]

# Prerequisites

you will need to install the following on your machine:

- [x] http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html[JDK 1.8]
- [x] https://maven.apache.org/download.cgi[Maven 3.3.6 or higher]
- [x] https://www.virtualbox.org/wiki/Downloads[Virtualbox 5.0 or higher]
- [x] https://github.com/minishift/minishift[Minishift 1.0.0.Beta3]
- [x] https://github.com/openshift/origin/releases/tag/v1.4.1[OpenShift Client 1.4.1]
- [x] https://forge.jboss.org/download[JBoss Forge]
- [x] http://developers.redhat.com/products/devstudio/download/?referrer=jbd[JBoss Developer Studio 10 (optional)]

# Installation of OpenShift

In order to use OpenShift platform on your laptob, we will use the Minishift Go clientApplication which has been created from the Minikube project of Kubernetes. It extends the features proposed by the Kubernetes client to package/deploy
OpenShift within a VM machine. Different hypervisors are supported as Virtualbox, xhyve & VMWare. You can find more information about Minishift like also how to install it from the project:
https://github.com/minishift/minishift

We will configure the VM on the machine using Virtualbox as Hypervisor, the version of OpenShift used is `1.4.1`. It is the default verison used by minishift 1.0.0.Beta3
To create the Virtual Machine, open a Terminal and execute this command.

[source]
----
minishift start --memory=4000 --vm-driver=virtualbox

Starting local OpenShift instance using 'virtualbox' hypervisor...
Provisioning OpenShift via '/Users/chmoulli/.minishift/cache/oc/v1.4.0-rc1/oc [cluster up --use-existing-config --host-config-dir /var/lib/minishift/openshift.local.config --host-data-dir /var/lib/minishift/hostdata]'
-- Checking OpenShift client ... OK
-- Checking Docker client ... OK
-- Checking Docker version ... OK
-- Checking for existing OpenShift container ...
   Deleted existing OpenShift container
-- Checking for openshift/origin:v1.4.0-rc1 image ... OK
-- Checking Docker daemon configuration ... OK
-- Checking for available ports ... OK
-- Checking type of volume mount ...
   Using Docker shared volumes for OpenShift volumes
-- Creating host directories ... OK
-- Finding server IP ...
   Using 192.168.64.25 as the server IP
-- Starting OpenShift container ...
   Starting OpenShift using container 'origin'
   Waiting for API server to start listening
   OpenShift server started
-- Removing temporary directory ... OK
-- Server Information ...
   OpenShift server started.
   The server is accessible via web console at:
       https://192.168.99.101:8443

   To login as administrator:
       oc login -u system:admin
----

Next, we will provide more rights for the admin `default` user in order to let it to access the different projects/namespaces to manage the resources.
This step is only required if you use the new Minishift client (>= 1.0.0.Beta3).

You can retrieve the provate address of the VM using the `minishift ip` command.

[source]
----
oc login https://$(minishift ip):8443 -u system:admin
oc adm policy add-cluster-role-to-user cluster-admin admin
oc login -u admin -p admin
oc project default
----

Remark : Optionally you can install OpenShift templates in order to have more examples to play on the platform as WildFly Server, MongoDb, MySQL Server, ...

[source]
----
export currentDir=$(pwd)
cd $TEMP_DIR
git clone https://github.com/openshift/openshift-ansible.git
cd openshift-ansible/roles/openshift_examples/files/examples/latest/
for f in image-streams/image-streams-centos7.json; do cat $f | oc create -n openshift -f -; done
for f in db-templates/*.json; do cat $f | oc create -n openshift -f -; done
for f in quickstart-templates/*.json; do cat $f | oc create -n openshift -f -; done
cd $currentDir
----

# Goals

The goal of this lab is to :

- Create a Microservices Java application that we will deploy within a virtualized environment managed by OpenShift,
- Externalize the configuration using https://kubernetes.io/docs/user-guide/configmap/[Kubernetes Config Map],
- Package/Deploy the project in OpenShift,
- Simplify the development of the application using JBoss Forge technology
- Implements the circuit broker pattern

The project will contain 3 modules; a web static Front end, a backend service exposed by the WildFly Swarm Java Container & a MySQL database.
The JPA layer is managed by Hibernate with the help of the module WildFly JPA. The front end is a AngularJS application.

Each module will be packaged and deployed as a Docker image on OpenShift. The OpenShift Source to Image Tool (= https://docs.openshift.com/enterprise/3.2/creating_images/s2i.html[S2I]) will be used for that purpose.
It will use the Java S2I Docker image responsible to build the final Docker image of your project using the source code of the maven module uploaded to the openshift platform.
This step will be performed using the https://maven.fabric8.io/[Fabric8 Maven Plugin]. This Maven plugin is a Java Kubernetes/OpenShift client able to communicate with the OpenShift platform using the REST endpoints
in order to issue the commands allowing to build aproject, deploy it and finally launch a docker process as a pod.

The project will be developed using Java IDE Tool like "IntelliJ, JBoss Developer Studio" while the JBoss Forge tool will help us to design the Java application, add the required dependencies,
 populate the Hibernate in order to:

- Create the REST Service
- Modelize the JPA Entity & the model
- Scaffold the AngularJS application

# Project creation

We will follow the following steps in order to create the maven project containing the modules of our application. Some prerequisites are required like JBoss Forge.
The first to be done is to git clone locally the project

  git clone https://github.com/redhat-microservices/lab_swarm-openshift.git
  cd lab_swarm-openshift

## All in one

The following script (if you want) can help you to setup the full project in one step. We invite you to first look to the decomposed steps in order to build the project step-by-step before
to use it.

[source]
----
 ./scripts/setup.sh
----

## Decomposed steps

### Parent project

. Open a terminal where we will create the snowcamp project
. Create the parent maven project
+
[source]
----
mvn archetype:generate -DarchetypeGroupId=org.codehaus.mojo.archetypes \
                       -DarchetypeArtifactId=pom-root \
                       -DarchetypeVersion=RELEASE \
                       -DinteractiveMode=false \
                       -DgroupId=org.cdstore \
                       -DartifactId=project \
                       -Dversion=1.0.0-SNAPSHOT
mv project snowcamp && cd snowcamp
----

### Catalog CD project

. Next create the `cdservice` maven module using the following JBoss Forge command where the stack to be used is Java EE. JBoss Forge will create
  a new maven module, configure the pom.xml file. The following command must be executed within the Forge shell or by passing the command using this convention
  `forge -e "..."` where `...` correspobds to a Forge command.
+
[source]
----
project-new --named cdservice --stack JAVA_EE_7
----

. Setup the JPA project where the provider used is `Hibernate`, the database `MYSQL` which corresponds to the dialect to be configured within the persistence file of Hibernate.
  Specify also the datasource and the persistent-unit name. All these parameters will be used by the Forge addon to populate the file persistence.xml under the directory META-INF
+
[source]
----
# Define PostgreSQL DB
jpa-setup --jpa-provider hibernate --db-type MYSQL --data-source-name java:jboss/datasources/CatalogDS --persistence-unit-name cdservice-persistence-unit
[source]
----

. Create a Catalog Java (but also entity) class where the fields will be defined as such. It is not required to define the field with the PRIMARY key as it will be created
  by default by the JBoss Forge command.
+
[source]
----
jpa-new-entity --named Catalog
jpa-new-field --named artist --target-entity org.cdservice.model.Catalog
jpa-new-field --named title --target-entity org.cdservice.model.Catalog
jpa-new-field --named description --length 2000 --target-entity org.cdservice.model.Catalog
jpa-new-field --named price --type java.lang.Float --target-entity org.cdservice.model.Catalog
jpa-new-field --named publicationDate --type java.util.Date --temporalType DATE --target-entity org.cdservice.model.Catalog
----

. As we target to communicate with a MySQL Databae, the mysql JDBC Java driver should be added to the pom definition of the module
  using this command
+
[source]
----
project-add-dependencies mysql:mysql-connector-java:5.1.40
----

. As we would like to expose our Catalog of CDs as a Service published behind a REST endopiint, we will use another +JBoss Forge command responsible
  to create a Rest Application and the Rest Service.
+
[source]
----
rest-generate-endpoints-from-entities --targets org.cdservice.model.*
----

. We are almost set. The last step of this module section will consist to use this JBoss Forge scaffold command.
  This command will populate the Web Front end which is a JavaScript AngularJS 1 project. This Front contains the screens
  required to perform the CRUD operations by calling the REST service `http://myservice.com/rest/catalogs`
+
[source]
----
scaffold-setup --provider AngularJS
scaffold-generate --provider AngularJS --generate-rest-resources --targets org.cdservice.model.*
----

. As we want that our `cdservice` can be bootstrapped using the WildFly Swarm Java Microservices container, we will issue this Jboss Forge command
  which will setup the maven module as a WildFly Swarm project and scan the project to detect the fractions to be included (Datasource, ...)
+
[source]
----
wildfly-swarm-setup
wildfly-swarm-detect-fractions --depend --build
----

. As the service will be called from a resources which is not running from the same HTTP Server and domain, a REST filter should be created to add the CORS Headers
+
[source]
----
cd cdservice
rest-new-cross-origin-resource-sharing-filter
----

. Now, we will add the Fabric8 Maven Plugin which is our client to comminicate with OpenShift by issuying this command
+
[source]
----
fabric8-setup
cd ..
----

. As the JBoss Fabric Forge Addon used will creta a project using the latest version of the Fabric8 plugin which hasn't been tested for this lab,
  We will Change the version of the Fabric8 Maven plugin from 3.2.9 to 3.1.92 like also specify the generator to be used.
  Add the generator wildfly-swarm that we will use
+
[source]
----
<plugin>
   <groupId>io.fabric8</groupId>
   <artifactId>fabric8-maven-plugin</artifactId>
   <version>3.1.92</version>
   <executions>
     <execution>
       <id>fmp</id>
       <goals>
         <goal>resource</goal>
         <goal>helm</goal>
         <goal>build</goal>
       </goals>
     </execution>
   </executions>
   <configuration>
     <generator>
       <includes>
         <include>wildfly-swarm</include>
       </includes>
     </generator>
   </configuration>
 </plugin>
----

### Configure the datasource

. To be able to use the project locally or on OpenShift, we will define different datasources and JDBC driver.
  Add a folder `src/main/config` containing a `project-stages.yaml` file. This file will contain the definition of the datasources
  that WildFly Swarm will use when Hibernate to try to call the database.
. Configure the datasource to use the H2 in-memory database with `ExampleDS` as datasource name
+
[source]
----
swarm:
  datasources:
    data-sources:
      ExampleDS:
        driver-name: h2
        connection-url: jdbc:h2:mem:test;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE
        user-name: sa
        password: sa
#      CatalogDS:
#        driver-name: mysql
#        connection-url: jdbc:mysql://localhost:3306/catalogdb
#        user-name: mysql
#        password: mysql
----

. Next, Copy/paste the generated persistence.xml file under the folder `src/main/config/META-INF` and change the datasource name like the dialect to be used
  within the persistence file.
+
[source]
----
<jta-data-source>java:jboss/datasources/ExampleDS</jta-data-source>
<property name="hibernate.dialect" value="org.hibernate.dialect.H2Dialect"/>
----

. Define a maven profile within the `pom.xml` file where we will tell to maven to copy the `src/main/config` content to the target folder `src/main/resources`
  when the project will be compiled. Declare also the h2 database dependency
+
[source]
----
 <profile>
      <id>local</id>
      <build>
        <resources>
          <resource>
            <directory>src/main/config</directory>
          </resource>
          <resource>
            <directory>src/main/resources</directory>
          </resource>
        </resources>
      </build>
      <dependencies>
        <dependency>
          <groupId>com.h2database</groupId>
          <artifactId>h2</artifactId>
          <version>1.4.192</version>
        </dependency>
      </dependencies>
    </profile>
----

. Move the `persistence.xml` file from the `src/main/resources` directory to the target directory `src/main/config-openshift/META-INF`
. Create another profile called `openshift`
+
[source]
----
<profile>
      <id>openshift</id>
      <build>
        <resources>
          <resource>
            <directory>src/main/config-openshift</directory>
          </resource>
          <resource>
            <directory>src/main/resources</directory>
          </resource>
        </resources>
      </build>
    </profile>
----

. Move the `MySQL Maven dependency` from the pom.xml within the `openshift` profile as the MySQL database will only be used when the project will be deployed on OpenShift.
+
[source]
----
...
    <!-- Manually -->
    <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.40</version>
    </dependency>
----

### Store Front end

. Create the store front project & setup WildFly Swarm. We will specify the HTTP Container to be used which is here Undertow
+
[source]
----
project-new --named cdstorefrontend --stack JAVA_EE_7 --type wildfly-swarm --http-port 8081
wildfly-swarm-add-fraction --fractions undertow
----

. As the web content has been created/populated previously, we will copy it here.
+
[source]
----
mv ../cdservice/src/main/webapp/ src/main/


# Keep empty src/main/webapp/WEB-INF
mkdir ../cdservice/src/main/webapp
mkdir ../cdservice/src/main/webapp/WEB-INF
----

. Change the version of the Fabric8 Maven plugin from 3.2.9 to 3.1.92
. Add the generator wildfly-swarm that we will use
+
[source]
----
<plugin>
        <groupId>io.fabric8</groupId>
        <artifactId>fabric8-maven-plugin</artifactId>
        <version>3.1.92</version>
        <executions>
          <execution>
            <id>fmp</id>
            <goals>
              <goal>resource</goal>
              <goal>helm</goal>
              <goal>build</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <generator>
            <includes>
              <include>wildfly-swarm</include>
            </includes>
          </generator>
        </configuration>
      </plugin>
----

## Build and deploy

### Build and run locally

. Open 2 terminal in order to start the front & backend
. cd `cdservice`

  mvn wildfly-swarm:run -Plocal

. cd `cdstorefront`

  mvn wildfly-swarm:run

. Open project within your browser `http://localhost:8081/index.html`

### Deploy on OpenShift

### Setup My SQL Database

. Verify first that you are well connected to Openshift

  oc status

. Create the snowcamp namespace/project

  oc new-project snowcamp

. Create the MySQL application using the OpenShift MySQL Template
+
[source]
----
  oc new-app --template=mysql-ephemeral \
      -p MYSQL_USER=mysql \
      -p MYSQL_PASSWORD=mysql \
      -p MYSQL_DATABASE=catalogdb
----

. Next, check if the Database is up and alive
+
[source]
----
export pod=$(oc get pod | grep mysql | awk '{print $1}')
oc rsh $pod
mysql -u $MYSQL_USER -p$MYSQL_PASSWORD -h $HOSTNAME $MYSQL_DATABASE

mysql> connect catalogdb;
Connection id:    1628
Current database: catalogdb

mysql> SELECT t.* FROM catalogdb.Catalog t;
----

### Externalize the Datasource

[TODO] Describe what we will do here from a general point of view

. Create under the directory `src/main/fabric8` of the `cdservice` maven module the `configmap.yml` file which contains the definition of the project-stages.yml
+
[source]
----
metadata:
  name: ${project.artifactId}
data:
  project-stages.yml: |-
    swarm:
      datasources:
        data-sources:
          CatalogDS:
            driver-name: mysql
            connection-url: jdbc:mysql://mysql:3306/catalogdb
            user-name: mysql
            password: mysql
----

Remark: As you can see, the hostname defined for the connection-url corresponds also to the `mysql` service published on OpenShift (`oc get svc/mysql`). This name will be resolved by the internal DNS server exposed by OpenShift
        when the application will issue a request to this machine.

. Add a `svc.yml` under the `src/main/fabric8` folder  where the target port is 8081 in order to create a service
+
[source]
----
apiVersion: v1
kind: Service
metadata:
  name: ${project.artifactId}
spec:
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8081
  type: ClusterIP
----

. Create a `route.yml` file under the `src/main/fabric8` to tell to OpenShift to create a route and specifies the target port whic is `8081`
+
[source]
----
apiVersion: v1
kind: Route
metadata:
  name: ${project.artifactId}
spec:
  port:
    targetPort: 8081
  to:
    kind: Service
    name: ${project.artifactId}
----

. Map the configMap to a volume that OpenShift will mount/attach to the pod when it will be created. So, create a deploymentconfig.yml file
+
[source]
----
spec:
  template:
    spec:
      containers:
        - volumeMounts:
            - name: config
              mountPath: /app/config
          env:
            - name: JAVA_OPTIONS
              value: "-Dswarm.project.stage.file=file:///app/config/project-stages.yml"
      volumes:
        - configMap:
            name: ${project.artifactId}
            items:
            - key: "project-stages.yml"
              path: "project-stages.yml"
          name: config
----

Remark : The location of the file to be used by WildFly Swarm is passed as JAVA_OPTINOS parameter

. Deploy the cd service project
+
[source]
----
mvn fabric8:deploy -Popenshift
----

. Check that you can access the REST endpoint of the service. Remark : you can get the route address using the command `oc get route/cdservice-snowcamp`

  http://cdservice-snowcamp.192.168.99.119.xip.io/rest/catalogs


### Externalize Front Service

[TODO] Describe what we will do here from a general point of view

. Create a `service.json` file under webapp folder of the cd front project & define the following key/value where the HOST address corresponds to the IP address used
  by your VM machine

  { "cd-service": "http://cdservice-snowcamp.192.168.99.119.xip.io/rest/catalogs/" }

. Create this `config.js` file within the directory scripts containing a $http.get request to access the content
  of the json file & fetch the key `cd-service`. This key will contain the hostname or service name to be accessed

[source]
----
angular.module('cdservice').factory('config', function ($http, $q) {
  var deferred = $q.defer();
  var apiUrl = null;
  $http.get("service.json")
    .success(function (data) {
      console.log("Resource : " + data['cd-service'] + ':CatalogId');
      deferred.resolve(data['cd-service']);
      apiUrl = data['cd-service'];
    })
    .error(function () {
      deferred.reject('could not find service.json ....');
    });

  return {
    promise: deferred.promise,
    getApiUrl: function () {
      return apiUrl;
    }
  };
});
----

. Modify the `scripts/services/CatalogFactory.js` to use the function `config` instead of the hard coded value

[source]
----
  return $resource(config.getApiUrl() + ':CatalogId', { CatalogId: '@id' }, {
----

. Update the routeProvider of the `app.js` script to access the service & setup a promise function as the call is asynchronous
[source]
----
      .when('/Catalogs',
      {
        templateUrl:'views/Catalog/search.html',
        controller:'SearchCatalogController',
        resolve: {
            apiUrl: function(config) {
              return config.promise;
            }
          }
      })
----

. Edit the app.html page to add the new script externalizing the URL

    <script src="scripts/services/config.js"></script>

. As we will deploy the CD Front project as a Servide that we will route externally from the host machine, we will create 2 OpenShift objects;
  one to configure the service exposed by the Kubernetes Api (gateway) and the other to configure the HA Proxy how to access the service from the host machine
. Add a `svc.yml` under the `src/main/fabric8` folder  where the target port is 8081 in order to create a service
+
[source]
----
apiVersion: v1
kind: Service
metadata:
  name: ${project.artifactId}
spec:
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8081
  type: ClusterIP
----

. Create a `route.yml` file under the `src/main/fabric8` to tell to OpenShift to create a route
+
[source]
----
apiVersion: v1
kind: Route
metadata:
  name: ${project.artifactId}
spec:
  port:
    targetPort: 8080
  to:
    kind: Service
    name: ${project.artifactId}
----

. Deploy the cd front project
+
[source]
----
mvn fabric8:deploy
----

. Check that you can access the HTML page of the Front. Remark : you can get the route address using the command `oc get route/cdfront-snowcamp`

  http://cdfront-snowcamp.192.168.99.119.xip.io/


## TO BE REVIEWED

. Forward the traffic from the service to the host using `port-forwarding` command

[source]
----
export pod=$(oc get pod | grep mysql | awk '{print $1}')
oc port-forward $pod 3306:3306
----

. Insert some records (if the table has been created !)
+
[source]
----
INSERT INTO Catalog (id, artist, description, price, publicationDate, title) VALUES (1,"ACDC","Australian hard rock band", 15.0, '1980-07-25', "Back in Black");
INSERT INTO Catalog (id, artist, description, price, publicationDate, title) VALUES (2,"Abba","Swedish pop music group", 12.0, '1976-10-11', "Arrival");
INSERT INTO Catalog (id, artist, description, price, publicationDate, title) VALUES (3,"Coldplay","British rock band ", 17.0, '2008-07-12', "Viva la Vida");
INSERT INTO Catalog (id, artist, description, price, publicationDate, title) VALUES (4,"U2","Irish rock band ", 18.0, '1987-03-09', "The Joshua Tree");
INSERT INTO Catalog (id, artist, description, price, publicationDate, title) VALUES (5,"Metallica","Heavy metal band", 15.0, '1991-08-12', "Black");
----


## Enable circuit breaker

. First of all you need to setup Turbine server and Hystrix dashboard:
+
[source]
----
oc create -f http://repo1.maven.org/maven2/io/fabric8/kubeflix/turbine-server/1.0.28/turbine-server-1.0.28-openshift.yml
oc policy add-role-to-user admin system:serviceaccount:snowcamp:turbine
oc expose service turbine-server

oc create -f http://repo1.maven.org/maven2/io/fabric8/kubeflix/hystrix-dashboard/1.0.28/hystrix-dashboard-1.0.28-openshift.yml
oc expose service hystrix-dashboard --port=8080
----


. Then you can create HystrixComands such as the one defined in org/cdservice/model/GetCatalogListCommand.java.

